{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97512327-3905-40a0-be99-211ae8189131",
   "metadata": {},
   "source": [
    "## Tutorial 3: Training Faster-RCNN model for Chest X-ray Abnormalities Detection\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This is a quick walkthrough notebook to demonstrate how to train Faster-RCNN detection model on Chest X-ray Abnormalities Dataset using PyTorch!\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "- [Imports](#Imports)\n",
    "- [Load VinBigData Chest X-ray Abnormalities Detection Dataset](#Load-VinBigData-Chest-X-ray-Abnormalities-Detection-Dataset)\n",
    "- [Define dataloaders](#Define-dataloaders)\n",
    "- [Define loss](#Define-helper-class-for-loss-calculation)\n",
    "- [Load model](#Load-model)\n",
    "- [Start training loop](#start-training-loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99194c38-6ae9-4b51-bafe-e254a0e61de2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc449e8b-3240-4cab-91cb-e3451a3419e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from farabio.data.biodatasets import VinBigDataset\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637d75b-8d7f-4f06-8004-44e217aff2cc",
   "metadata": {},
   "source": [
    "#### Load  VinBigData Chest X-ray Abnormalities Detection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfdc8564-1893-41b3-a97a-8ff1d458b223",
   "metadata": {},
   "outputs": [],
   "source": [
    "_path = \"/home/data/07_SSD4TB/public-datasets/vinbigdata-chest-xray-abnormalities-detection\"\n",
    "\n",
    "train_dataset = VinBigDataset(_path, transform=None, download=False, mode=\"train\", show=False)\n",
    "valid_dataset = VinBigDataset(_path, transform=None, download=False, mode=\"val\", show=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761846e3-ccc4-40bf-bac7-33e32bf02765",
   "metadata": {},
   "source": [
    "#### Define dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "426b32c9-38d7-4b80-b20b-93601a5bcea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    num_workers=32,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "valid_data_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=32,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec39f2de-159e-4b5a-889c-1e8f4685fde0",
   "metadata": {},
   "source": [
    "#### Define helper class for loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f40e345-af01-45ce-95d9-15b6ce03f62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Averager:\n",
    "    def __init__(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0\n",
    "\n",
    "    def send(self, value):\n",
    "        self.current_total += value\n",
    "        self.iterations += 1\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        if self.iterations == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1.0 * self.current_total / self.iterations\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_total = 0.0\n",
    "        self.iterations = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a4c87-94ca-4761-b66d-cc431b8d713c",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e2201bb-31d0-43e1-a032-cbdbfc17f3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 6 GPUs!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "multi_gpu = True\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = train_dataset.num_classes + 1  # + 1 for background\n",
    "\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "if torch.cuda.device_count() > 1 and multi_gpu is True:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d93d22b-d94a-4bab-8ea6-542cb91e65d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Start training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f75e254b-d4ad-4e06-a85d-e95264acd228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #50 loss: 0.3085425794124603\n",
      "Iteration #100 loss: 0.5478842258453369\n",
      "Iteration #150 loss: 0.4258052706718445\n",
      "Iteration #200 loss: 0.2777027189731598\n",
      "Iteration #250 loss: 0.2505597174167633\n",
      "Epoch #0 loss: 0.3619921253859133\n",
      "Iteration #300 loss: 0.31739604473114014\n",
      "Iteration #350 loss: 0.41544443368911743\n",
      "Iteration #400 loss: 0.30315133929252625\n",
      "Iteration #450 loss: 0.3148651719093323\n",
      "Iteration #500 loss: 0.24818390607833862\n",
      "Iteration #550 loss: 0.3079833388328552\n",
      "Epoch #1 loss: 0.347546909922275\n",
      "Iteration #600 loss: 0.3068886399269104\n",
      "Iteration #650 loss: 0.38821980357170105\n",
      "Iteration #700 loss: 0.2054576724767685\n",
      "Iteration #750 loss: 0.3481389284133911\n",
      "Iteration #800 loss: 0.2565847933292389\n",
      "Epoch #2 loss: 0.33110840413449466\n",
      "Iteration #850 loss: 0.3899312913417816\n",
      "Iteration #900 loss: 0.35357269644737244\n",
      "Iteration #950 loss: 0.32706546783447266\n",
      "Iteration #1000 loss: 0.3869641125202179\n",
      "Iteration #1050 loss: 0.21673503518104553\n",
      "Iteration #1100 loss: 0.338126540184021\n",
      "Epoch #3 loss: 0.3287309571776701\n",
      "Iteration #1150 loss: 0.2647096812725067\n",
      "Iteration #1200 loss: 0.209655299782753\n",
      "Iteration #1250 loss: 0.28139761090278625\n",
      "Iteration #1300 loss: 0.3673282563686371\n",
      "Iteration #1350 loss: 0.3458060026168823\n",
      "Epoch #4 loss: 0.3200093915928965\n",
      "Iteration #1400 loss: 0.2980700433254242\n",
      "Iteration #1450 loss: 0.24559985101222992\n",
      "Iteration #1500 loss: 0.398821622133255\n",
      "Iteration #1550 loss: 0.4273016154766083\n",
      "Iteration #1600 loss: 0.26512083411216736\n",
      "Iteration #1650 loss: 0.2448403239250183\n",
      "Epoch #5 loss: 0.3193448923960112\n",
      "Iteration #1700 loss: 0.24169345200061798\n",
      "Iteration #1750 loss: 0.30709517002105713\n",
      "Iteration #1800 loss: 0.31985023617744446\n",
      "Iteration #1850 loss: 0.3102235794067383\n",
      "Iteration #1900 loss: 0.27542465925216675\n",
      "Epoch #6 loss: 0.3157201996747998\n",
      "Iteration #1950 loss: 0.26590558886528015\n",
      "Iteration #2000 loss: 0.27121061086654663\n",
      "Iteration #2050 loss: 0.388118177652359\n",
      "Iteration #2100 loss: 0.2315744310617447\n",
      "Iteration #2150 loss: 0.2755761444568634\n",
      "Iteration #2200 loss: 0.3327944576740265\n",
      "Epoch #7 loss: 0.31482708664691966\n",
      "Iteration #2250 loss: 0.2769106924533844\n",
      "Iteration #2300 loss: 0.295138418674469\n",
      "Iteration #2350 loss: 0.475500226020813\n",
      "Iteration #2400 loss: 0.33869585394859314\n",
      "Iteration #2450 loss: 0.3207103908061981\n",
      "Epoch #8 loss: 0.31223686277002527\n",
      "Iteration #2500 loss: 0.2763749957084656\n",
      "Iteration #2550 loss: 0.2482030689716339\n",
      "Iteration #2600 loss: 0.26101812720298767\n",
      "Iteration #2650 loss: 0.2273288518190384\n",
      "Iteration #2700 loss: 0.3249484896659851\n",
      "Iteration #2750 loss: 0.261581689119339\n",
      "Epoch #9 loss: 0.31622466461166093\n",
      "Time taken to Train the model :00:32:09.69\n"
     ]
    }
   ],
   "source": [
    "num_epochs =  10 # change here to try train more\n",
    "loss_hist = Averager()\n",
    "itr = 1\n",
    "lossHistoryiter = []\n",
    "lossHistoryepoch = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_hist.reset()\n",
    "    \n",
    "    for images, targets, image_ids in train_data_loader:\n",
    "        \n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)  \n",
    "        \n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        loss_hist.send(loss_value)\n",
    "        lossHistoryiter.append(loss_value)\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if itr % 50 == 0:\n",
    "            print(f\"Iteration #{itr} loss: {loss_value}\")\n",
    "\n",
    "        itr += 1\n",
    "    \n",
    "    # update the learning rate\n",
    "    if lr_scheduler is not None:\n",
    "        lr_scheduler.step()\n",
    "    lossHistoryepoch.append(loss_hist.value)\n",
    "    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")   \n",
    "    \n",
    "end = time.time()\n",
    "hours, rem = divmod(end-start, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(\"Time taken to Train the model :{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('fo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb51149769cf347739d637c1ae5a96d7da2012bfd617c152fc0027c8937b7877"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
